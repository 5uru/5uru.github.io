<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Papiers de Recherche - Jonathan Suru</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
  <h1>Jonathan Suru</h1>
  <nav>
    <a href="index.html">Accueil</a> |
    <a href="projects.html">Projets</a> |
    <a href="papers.html">Papiers de Recherche</a> |
    <a href="about.html">À propos</a>
  </nav>
</header>

<main>
  <h2>Papiers de Recherche</h2>
  <p>Cette section est dédiée à mes reproductions et implémentations de travaux académiques en IA.</p>

  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/gcn">Implémentation d'un Réseau de Convolution sur Graphes (GCN) avec TinyGrad.</a></h3>
    <p>J'ai implémenté un **GCN (Graph Convolutional Network)** pour détecter des communautés dans le <em>dataset Zachary Karate Club</em>. Le modèle utilise deux couches de convolution graphique, chacune suivie d'une couche linéaire et d'une activation <code>tanh</code>, terminé par un classifieur linéaire. Les caractéristiques des nœuds sont initialement codées en <strong>one-hot</strong>, et la matrice d'adjacence est normalisée avec des boucles auto-connectées. L'entraînement s'est effectué sur <strong>100 époques</strong> avec l'optimiseur <strong>Adam</strong>, atteignant une <strong>accuracy de 100%</strong> dès la 20e époque.</p>
  </article>
  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/adamW_optimizer">Implémentation de l'Optimiseur <strong>Lion</strong> avec TinyGrad.</a></h3>
    <p>J'ai implémenté l'optimiseur <strong>Lion</strong> (basé sur l'article <a href="https://arxiv.org/abs/2302.06675 ">Symbolic Discovery of Optimization Algorithms</a>) pour entraîner un CNN sur le dataset <em>MNIST</em>. Contrairement à Adam, Lion n'utilise qu'un seul paramètre de moment et applique une opération <code>sign()</code> sur les mises à jour des poids, réduisant la mémoire utilisée.</p>
    <p>Les résultats montrent une <strong>convergence extrêmement rapide</strong> : la perte chute de <strong>2.0</strong> à <strong>0.02</strong> en 5 000 étapes, tandis que l'accuracy atteint <strong>99%</strong> dès les premières centaines d'étapes.</p>
  </article>
  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/adamW_optimizer">Implémentation d'un Optimiseur AdamW avec TinyGrad.</a></h3>
    <p>J'ai implémenté une version personnalisée de l'optimiseur <strong>AdamW</strong> pour entraîner un réseau de neurones convolutif (CNN) sur le dataset <em>MNIST</em>.</p>
    <p>Les résultats montrent une <strong>convergence rapide</strong> : la perte initiale de <strong>2.0</strong> chute à <strong>0.02</strong> en 5 000 étapes, tandis que l'accuracy atteint <strong>99%</strong> dès les premières centaines d'étapes.</p>
  </article>
  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/rbm">Implémentation d'une Machine de Boltzmann Restreinte (RBM) avec TinyGrad.</a></h3>
    <p>J’ai implémenté une <strong>RBM (Restricted Boltzmann Machine)</strong> avec <em>TinyGrad</em> pour recommander des livres via le dataset <em>GoodBooks-10k</em>. Le modèle compte <strong>64 unités cachées</strong> et <strong>17 000 unités visibles</strong>, entraîné sur <strong>150 000 notations</strong> en <strong>5 époques</strong> avec un taux d’apprentissage de <strong>0.6</strong> et des lots de <strong>100 échantillons</strong>.</p>
    <p>Les résultats montrent une <strong>convergence stable</strong> : l’erreur moyenne diminue de <strong>0.029 à 0.005</strong>, tandis que l’énergie libre augmente de <strong>150 à 280</strong>.</p>
  </article>
  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/vgg">Implémentation du Réseau VGG avec TinyGrad.</a></h3>
    <p>J'ai implémenté une version simplifiée du réseau <strong>VGG</strong> pour classer les images de <strong>CIFAR-10</strong>, entraîné en <em>7000 étapes</em> avec <strong>Adam (lr=0.001)</strong> et des <strong>lots de 128 images</strong>.</p>
    <p>Le modèle atteint <strong>75 % d'accuracy</strong> grâce à <em>8 couches convolutionnelles</em> et un <em>dropout</em>, illustrant les performances sur ce dataset.</p>  </article>

  <article>

    <h3><a href="https://github.com/5uru/Stream/tree/main/alexnet">AlexNet avec TinyGrad : Une exploration du deep learning minimaliste.</a></h3>
    <p>Dans ce projet, j'ai réimplementé l'architecture emblématique d'AlexNet en utilisant <a href="https://github.com/tinygrad/tinygrad " target="_blank">TinyGrad</a>, un framework minimaliste pour le deep learning. Entraîné sur le dataset CIFAR-10 avec un optimiseur Adam (learning rate=0.001) et des lots de 128 images, le modèle atteint une précision de <strong>75%</strong> sur les données de test après 7000 étapes d'entraînement.</p>
  </article>
</main>

<footer>
  <p><span class="copyleft">&copy;</span> 2024 Jonathan Suru. This work is free.</p>
</footer>
<!-- 100% privacy-first analytics -->
<script data-collect-dnt="true" async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=true" alt="" referrerpolicy="no-referrer-when-downgrade"/></noscript>
</body>
</html>