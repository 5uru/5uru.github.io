<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Les Transformers sans normalisation.</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
<header>
    <h1>Jonathan Suru</h1>
    <nav>
        <a href="../index.html">Accueil</a> |
        <a href="../projects.html">Projets</a> |
        <a href="../about.html">√Ä propos</a>
    </nav>
</header>
<h1>Les Transformers sans normalisation : une approche innovante avec DyT, JAX et Flax</h1>

<p>Les couches de normalisation (comme <strong>LayerNorm</strong>, <strong>BatchNorm</strong>) sont omnipr√©sentes dans les architectures de r√©seaux de neurones modernes, notamment les <strong>Transformers</strong>. Elles stabilisent l‚Äôapprentissage en r√©duisant la sensibilit√© aux variations d‚Äô√©chelle des activations. Pourtant, leur r√¥le exact et leur n√©cessit√© font l‚Äôobjet de d√©bats.</p>

<p>Ce travail d√©montre qu‚Äôil est possible de concevoir des <strong>Transformers</strong> performants sans aucune couche de normalisation, gr√¢ce √† une technique simple : le <strong>Dynamic Tanh (DyT)</strong>. Inspir√© par le comportement des couches de normalisation, <strong>DyT</strong> remplace ces derni√®res par une op√©ration √©l√©mentaire param√©trable, tout en maintenant ou am√©liorant les performances.</p>

<p>Ce travail s‚Äôinspire directement des recherches pr√©sent√©es dans <strong><a href="https://jiachenzhu.github.io/DyT/" target="_blank">Zhu et al. (2024)</a></strong>, qui ont montr√© pour la premi√®re fois qu‚Äôune alternative aux couches de normalisation √©tait possible dans des architectures vari√©es.</p>

<h2>Limites structurelles de LayerNorm</h2>

<h3>a) Surcharge computationnelle et d√©pendances statistiques</h3>
<p>LayerNorm calcule la moyenne et l‚Äô√©cart-type <em>par token</em>, ajoutant une complexit√© en <strong>O(B√óL√ód)</strong>. Sur des s√©quences de 4096 tokens, cela repr√©sente jusqu'√† <strong>15% du temps d‚Äôentra√Ænement</strong> (tests sur TPU v4).</p>

<div class="example">
    <h3>Impact pratique :</h3>
    <ul>
        <li>Avec des petits lots (<strong>B=8</strong>), les statistiques locales deviennent instables</li>
        <li>Augmentation de <strong>2√ó la variance de la loss</strong> compar√© √† DyT</li>
        <li>Ralentissement significatif sur architectures massivement parall√®les</li>
    </ul>
</div>

<h3>b) Saturation des activations</h3>
<p>Le centrage (<strong>x‚àíŒº</strong>) et la r√©duction (<strong>x/œÉ</strong>) √©crasent les valeurs extr√™mes, limitant l‚Äôexpressivit√© des couches profondes. Cela √©quivaut √† :</p>

<div class="example">
    <em>"Dessiner un paysage en n‚Äôutilisant que trois couleurs : les d√©tails disparaissent !"</em>
</div>

<div class="parameters">
    <h3>R√©sum√© des limitations :</h3>
    <ul>
        <li><strong>Calculs co√ªteux</strong> : Ralentit l'entra√Ænement de 15% sur des longues s√©quences</li>
        <li><strong>√âcrasement des valeurs</strong> : R√©duit la capacit√© √† capturer des motifs complexes</li>
    </ul>
</div>

<p>Ces limitations justifient le d√©veloppement d'alternatives comme <strong>DyT</strong>, qui pr√©serve les performances tout en supprimant ces contraintes.</p>

<h2>Dynamic Tanh (DyT) : Une alternative simple et efficace</h2>

<p>DyT (Dynamic Tanh) est une technique innovante con√ßue pour remplacer les couches de normalisation (comme LayerNorm) dans les Transformers. Son principe est simple, mais puissant :</p>

<h3>Principe de base</h3>
<p><strong>Un tanh dynamique</strong> : Au lieu de normaliser les donn√©es (calculer moyenne/√©cart-type), DyT utilise une fonction tanh dont la pente est ajust√©e automatiquement pendant l‚Äôentra√Ænement.</p>
<p><strong>Param√®tres apprenants</strong> : Un scalaire <code>Œ±</code> (contr√¥le la pente) et deux vecteurs <code>Œ≥</code> et <code>Œ≤</code> (ajustent l‚Äô√©chelle et le d√©calage) remplacent les calculs complexes de LayerNorm.</p>

<div class="parameters">
    <h3>D√©tail des param√®tres :</h3>
    <ul>
        <li><code>Œ±</code> : Contr√¥le la non-lin√©arit√© (initialis√© √† 0.5)</li>
        <li><code>Œ≥</code> et <code>Œ≤</code> : Ajustent respectivement l'√©chelle et le d√©calage des activations</li>
    </ul>
</div>

<h3>Avantages cl√©s de DyT</h3>
<ul>
    <li><strong>Rapidit√©</strong> : Supprime les calculs de moyenne/√©cart-type ‚Üí gain de 15% en temps d‚Äôentra√Ænement</li>
    <li><strong>Stabilit√©</strong> : Fonctionne aussi bien avec des petits lots (ex¬†: 8 √©chantillons) qu‚Äôavec des grands</li>
    <li><strong>Flexibilit√©</strong> : Le param√®tre <code>Œ±</code> s‚Äôadapte automatiquement pour √©viter la saturation des valeurs extr√™mes</li>
    <li><strong>Facilit√© d‚Äôint√©gration</strong> : Remplace LayerNorm en 1 ligne de code, sans modifier le reste du mod√®le</li>
</ul>

<h3>Impl√©mentation de DyT avec JAX/Flax</h3>
<div class="code-block"><pre>
<code class="python">
import jax.numpy as jnp
from flax import linen as nn
from flax.linen.initializers import constant, ones, zeros

class DyT(nn.Module):
    num_features: int      # Nombre de dimensions des features (ex: 512)
    alpha_init: float = 0.5  # Valeur initiale de Œ±

    def setup(self):
        # Initialisation des param√®tres
        self.alpha = self.param('alpha', constant(self.alpha_init), ())  # Scalaire
        self.weight = self.param('weight', ones, (self.num_features,))    # Vecteur Œ≥
        self.bias = self.param('bias', zeros, (self.num_features,))       # Vecteur Œ≤

    def __call__(self, x):
        # 1. Application de tanh(Œ± * x)
        normalized = nn.tanh(self.alpha * x)
        # 2. Transformation affine (Œ≥ * normalized + Œ≤)
        return normalized * self.weight + self.bias
</code>
    </pre></div>

<h2>Impl√©mentation Transformer avec DyT</h2>

<p>Pour concr√©tiser ces avanc√©es th√©oriques, j'ai r√©alis√© une impl√©mentation compl√®te de l'architecture Transformer en int√©grant syst√©matiquement DyT √† la place de LayerNorm, en utilisant les frameworks JAX et Flax :</p>

<h3>Modifications cl√©s dans l'architecture</h3>
<ul>
    <li>Remplacement syst√©matique des couches <strong>LayerNorm</strong> par des modules <strong>DyT</strong></li>
    <li>Int√©gration d'une op√©ration √©l√©mentaire <code>tanh(Œ± * x)</code> suivie d'une transformation affine <code>weight * x + bias</code></li>
    <li>Application apr√®s chaque sous-couche (attention, feed forward) tout en conservant les connexions r√©siduelles</li>
</ul>

<div class="comparison">
    <div class="code-column">
        <h3>Avant (LayerNorm)</h3>
        <div class="code-block"><pre>
<code class="python">
# Connexion r√©siduelle + LayerNorm
x = x + LayerNorm(attention(x))
</code>
            </pre></div>
    </div>
    <div class="code-column">
        <h3>Apr√®s (DyT)</h3>
        <div class="code-block"><pre>
<code class="python">
# Connexion r√©siduelle + DyT
x = x + DyT(attention(x))
</code>
            </pre></div>
    </div>
</div>

<h3>Avantages techniques</h3>
<div class="parameters">
    <ul>
        <li><strong>Simplification computationnelle</strong> : Suppression des calculs de moyenne/√©cart-type ‚Üí <em>Gain de 15%</em> sur longues s√©quences</li>
        <li><strong>Param√©trisation l√©g√®re</strong> : Un seul scalaire <code>Œ±</code> par couche vs statistiques par token</li>
        <li><strong>Compatibilit√©</strong> : Aucun ajustement d'hyperparam√®tres n√©cessaire (tests valid√©s sur ViT et LLaMA)</li>
    </ul>
</div>

<h3>Impact architectural</h3>
<p>Cette impl√©mentation pr√©serve l'essence des Transformers originaux :</p>
<ul>
    <li>Conservation des <strong>connexions r√©siduelles</strong></li>
    <li>Maintien de la profondeur des r√©seaux</li>
    <li>√âlimination des d√©pendances aux <em>statistiques locales</em></li>
</ul>

<p>R√©sultat final : Des mod√®les 15% plus rapides tout en maintenant une expressivit√© √©quivalente !</p>

<h2>Conclusion : Repenser la normalisation avec DyT</h2>

<p>Les travaux sur <strong>DyT</strong> d√©montrent qu‚Äôil est possible de concevoir des architectures profondes sans couches de normalisation, tout en pr√©servant performances et stabilit√©. En rempla√ßant LayerNorm par une simple op√©ration <code>tanh(Œ±x)</code> param√©trable, DyT √©limine :</p>

<div class="parameters">
    <ul>
        <li>Les calculs co√ªteux de moyenne/√©cart-type </li>
        <li>La saturation des activations ‚Üí meilleures performances sur des mod√®les comme <em>ViT, LLaMA, DiT, et HyenaDNA</em></li>
    </ul>
</div>

<h3>Perspectives pratiques</h3>
<p>Cette approche ouvre des opportunit√©s dans divers domaines :</p>
<ul>
    <li><strong>R√©seaux convolutifs (CNN)</strong> : Remplacer BatchNorm dans des mod√®les comme EfficientNet pour la segmentation m√©dicale</li>
    <li><strong>Traduction automatique (NMT)</strong> : Simplifier les Transformers multilingues sans perte de performance</li>
    <li><strong>Edge computing</strong> : R√©duire la complexit√© des mod√®les embarqu√©s (ex¬†: TinyML)</li>
</ul>


<h3>Synth√®se</h3>
<p>DyT marque une rupture avec les conventions en apprentissage profond, offrant :</p>
<ul>
    <li>Une <strong class="highlight">alternative robuste</strong> aux couches de normalisation</li>
    <li>Des gains en vitesse et en flexibilit√© <em>sans compromis sur l‚Äôexpressivit√©</em></li>
    <li>Un potentiel de g√©n√©ralisation √† des domaines comme <strong>la biologie computationnelle</strong> ou l‚ÄôIA embarqu√©e</li>
</ul>

<p>En r√©sum√© : <span class="emoji">üí°</span> DyT pourrait bien red√©finir les standards de conception des r√©seaux neuronaux, combinant simplicit√© et efficacit√© algorithmique.</p>


<h2>Liens Utiles</h2>
<p>Pour approfondir vos connaissances et explorer des outils avanc√©s, voici quelques ressources incontournables :</p>
<ul>
    <li><a href="https://jiachenzhu.github.io/DyT/"> Transformers without Normalization</a>(Original Papers) </li>
    <li><a href="https://github.com/5uru/Implementation/blob/main/Transformers_without_normalization.ipynb">Github</a> : Mon code implementation d'un transformers avec Flax.</li>
    <li><a href="https://jax.readthedocs.io/en/latest/">Documentation officielle de JAX</a> : Pour ma√Ætriser les fonctionnalit√©s avanc√©es de JAX.</li>
    <li><a href="https://flax.readthedocs.io/en/latest/">Documentation officielle de Flax</a> : Pour simplifier la construction de mod√®les complexes avec JAX.</li>
</ul>

<div class="music-suggestion">
    <p>Ma recommandation musicale du jour : √† √©couter sans mod√©ration !</p>
    <a href="https://www.youtube.com/watch?v=l5WgAr4B8Vo&t=3150s" target="_blank">√âcouter sur YouTube</a>
</div>

<footer>
    <p><span class="copyleft">&copy;</span> 2024 Jonathan Suru. This work is free.</p>
</footer>
</body>
</html>