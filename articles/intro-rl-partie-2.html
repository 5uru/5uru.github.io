<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:image" content="https://jonathansuru.me/thumbnail.png"/>
    <meta property="og:image:alt" content="Code Learn Share" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <title>Introduction Au Reinforcement Learning - Partie 2</title>

    <!-- Polyfill pour compatibilit√© ES6 -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- MathJax pour les formules math√©matiques -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Feuille de style externe -->
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
<header>
    <h1>Jonathan Suru</h1>
    <nav>
        <a href="../index.html">Accueil</a> |
        <a href="../projects.html">Projets</a> |
        <a href="../about.html">√Ä propos</a>
    </nav>
</header>
<h1>Introduction Au Reinforcement Learning - Partie 2</h1>


<p>Dans <a href="https://jonathansuru.me/articles/intro-rl-partie-1.html">mon pr√©c√©dent article</a>, j'ai pos√© les fondations de l'apprentissage par renforcement. Nous y avons vu comment un agent, tel un √©cureuil malin, pouvait apprendre √† naviguer dans un monde simple en utilisant des tables de valeurs (Q-tables). Cependant, nous avons √©galement mis en lumi√®re leur talon d'Achille : la "mal√©diction de la dimensionnalit√©". Face √† un monde complexe comme un jeu vid√©o ou la conduite d'une voiture, une table devient inutilisable.</p>

<p>Vous voil√† au bord d'une r√©volution. Celle qui a propuls√© le reinforcement learning sur le devant de la sc√®ne de l'intelligence artificielle. L'id√©e est g√©niale dans sa simplicit√© : remplacer la table rigide et limit√©e par un cerveau souple et puissant, un r√©seau de neurones profond. C'est la naissance du <strong>Deep Reinforcement Learning (DRL)</strong>.</p>

<p>Cet article est votre guide pour comprendre cette transition. Nous partirons de l'algorithme qui a tout chang√©, le Deep Q-Network (DQN), pour ensuite explorer comment le domaine a su s'adapter pour ma√Ætriser des actions continues et devenir plus stable et efficace que jamais.</p>

<hr>

<h2>Le Deep Q-Network (DQN)</h2>

<p>Imaginez que vous ne vouliez plus seulement apprendre une grille de \(10\times10\), mais ma√Ætriser un jeu comme <em>Breakout</em> ou <em>Space Invaders</em>, o√π l'√©tat n'est plus une simple coordonn√©e, mais une image enti√®re de \(84\times84\) pixels en couleurs. Le nombre d'√©tats possibles est \(256^{(84\times84\times3)}\), un nombre si grand qu'il d√©passe l'entendement. Une Q-table est non seulement inutilisable, elle est une absurdit√© conceptuelle.</p>

<p>Le <strong>Deep Q-Network (DQN)</strong>, introduit en 2013 par DeepMind dans leur article fondateur "Playing Atari with Deep Reinforcement Learning", a r√©solu ce probl√®me en mariant deux mondes : l'apprentissage par renforcement et l'apprentissage profond. La solution repose sur deux innovations majeures.</p>

<h3>1. La Fonction d'Approximation : Un R√©seau de Neurones √† la place de la Table</h3>

<p>Au lieu d'une table, DQN utilise un r√©seau de neurones convolutif (CNN), un type de r√©seau particuli√®rement dou√© pour comprendre les images. Ce r√©seau prend en entr√©e l'√©tat (l'image du jeu) et sort en direct les valeurs Q pour chaque action possible (gauche, droite, tirer...).</p>

<h3>2. L'Experience Replay : Apprendre du Pass√© pour Mieux Agir dans le Futur</h3>

<p>Un agent qui apprend en ligne, pas √† pas, est confront√© √† un probl√®me insidieux : les exp√©riences successives sont fortement corr√©l√©es. Si l'agent vient de se d√©placer √† droite, son prochain √©tat sera probablement aussi un √©tat o√π il est un peu plus √† droite. Apprendre √† partir de cette s√©quence continue, c'est comme essayer d'apprendre une nouvelle mati√®re en ne lisant que la m√™me page d'un livre encore et encore. On risque de sur-sp√©cialiser notre apprentissage √† une situation tr√®s r√©cente et passag√®re.</p>

<p>DQN introduit une m√©moire tampon, l'<strong>Experience Replay Buffer</strong>.</p>

<ul>
    <li>√Ä chaque √©tape, l'agent stocke sa transition \( (\text{√©tat},\, \text{action},\, \text{r√©compense},\, \text{nouvel √©tat},\, \text{termin√©}) \) dans ce buffer. Le buffer a une taille finie (par exemple, 1 million d'exp√©riences) et, lorsqu'il est plein, il commence √† √©craser les plus anciennes exp√©riences.</li>
    <li>Pour s'entra√Æner, l'agent ne prend pas la derni√®re exp√©rience. Au lieu de cela, il <strong>√©chantillonne au hasard un mini-lot (par exemple, 32 exp√©riences) de son pass√©</strong> dans ce buffer.</li>
</ul>

<p>C'est une r√©volution ! Cela brise la corr√©lation des donn√©es, un principe fondamental de l'apprentissage automatique. L'agent peut r√©apprendre de ses anciennes erreurs et succ√®s, encore et encore. Une exp√©rience rare mais cruciale (par exemple, perdre une vie) peut √™tre r√©-√©chantillonn√©e de nombreuses fois, s'assurant que l'agent en tire bien la le√ßon.</p>

<h3>L'Algorithme DQN en Bref :</h3>
<ol>
    <li>Initialiser le r√©seau de neurones \(Q\) (appel√© "Online Network" et un r√©seau cible gel√© \( \hat{Q} \) ("Target Network", avec les poids qui sont une copie de \(Q\)).</li>
    <li>Initialiser un buffer de m√©moire \(D\).</li>
    <li>Pour chaque √©tape :
        <ol type="a">
            <li>Choisir une action avec une politique Œµ-greedy (exploration vs exploitation).</li>
            <li>Ex√©cuter l'action et observer la r√©compense \(r\) et le nouvel √©tat \(s'\).</li>
            <li>Stocker la transition \((s, a, r, s', done)\) dans le buffer \(D\).</li>
            <li><strong>√âchantillonner un mini-lot al√©atoire de transitions depuis D.</strong></li>
            <li>Calculer la "cible" \(y\) pour l'apprentissage en utilisant le <strong>Target Network</strong> pour stabiliser l'apprentissage :
                $$                     y = \begin{cases}
                r & \text{si l'√©pisode est termin√©} \\
                r + \gamma \max_{a'} Q(s', a') & \text{sinon}
                \end{cases}
                $$                 </li>
            <li>Entra√Æner le <strong>Online Network</strong> sur ce mini-lot pour minimiser l'erreur quadratique moyenne (la <em>loss</em>) entre sa pr√©diction \(Q(s, a)\) et la cible \(y\) :
                $$                     \text{Loss} = \mathbb{E} \left[ (y - Q(s, a))^2 \right]
                $$                 </li>
            <li>Tous les \(C\) pas (par exemple, 1000), copier les poids de \(Q\) vers \( \hat{Q} \) pour mettre √† jour le r√©seau cible.</li>
        </ol>
    </li>
</ol>

<hr>

<h2>Le DQN : Une R√©volution avec ses Limites</h2>

<p>Le Deep Q-Network a √©t√© une v√©ritable bombe. Pour la premi√®re fois, un agent pouvait apprendre directement √† partir de pixels bruts, atteignant des performances surhumaines sur une multitude de jeux classiques d'Atari. Le grand m√©rite du DQN est d'avoir r√©solu le probl√®me de la "mal√©diction de la dimensionnalit√©" en utilisant un r√©seau de neurones comme fonction d'approximation, et d'avoir introduit l'Experience Replay pour un apprentissage stable et efficace.</p>

<p>Cependant, le DQN n'√©tait pas parfait. Loin de l√†. Les chercheurs ont rapidement identifi√© plusieurs faiblesses fondamentales :</p>

<ul>
    <li><strong>La limite des actions discr√®tes :</strong> C'est la plus grande limitation. Le DQN ne peut que choisir parmi un nombre fini d'actions (gauche, droite, tirer...). Il est totalement incapable de g√©rer des actions continues, comme "tourner le volant de 15.7 degr√©s" ou "acc√©l√©rer avec une force de 0.8". Cette restriction le rend inutile pour de nombreux probl√®mes du monde r√©el comme la robotique ou le contr√¥le de v√©hicules.</li>
    <li><strong>L'instabilit√© de l'apprentissage :</strong> M√™me avec ses innovations, l'entra√Ænement du DQN pouvait √™tre d√©licat. La "cible" qu'il cherchait √† atteindre bougeait √† chaque mise √† jour, un peu comme un chien qui court apr√®s sa propre queue, ce qui pouvait rendre l'apprentissage chaotique.</li>
    <li><strong>Le biais de surestimation :</strong> Le DQN avait tendance √† √™tre trop optimiste, √† croire que la valeur des actions √©tait meilleure qu'elle ne l'est en r√©alit√©.</li>
</ul>

<h3>La Famille DQN : Des Am√©liorations Successives</h3>

<p>Face √† ces d√©fauts, la communaut√© de la recherche n'a pas abandonn√© le DQN. Elle l'a am√©lior√©, cr√©ant toute une famille d'algorithmes de plus en plus performants, tous bas√©s sur l'id√©e originale du DQN. Voici les plus c√©l√®bres, pr√©sent√©es sans jargon technique :</p>

<ul>
    <li><strong>Double DQN (DDQN) :</strong> Pour corriger le biais de l'ami trop optimiste, DDQN a introduit une astuce simple : dissocier le choix de l'action de son √©valuation. Cela a permis de r√©duire drastiquement la surestimation et de rendre l'apprentissage plus fiable.</li>
    <li><strong>Dueling DQN :</strong> Pour mieux comprendre une situation, cet algorithme a appris √† r√©pondre √† deux questions : "Est-ce que cet √©tat est globalement bon ?" (valeur de l'√©tat) et "Quelle action est la meilleure *dans* cet √©tat ?" (valeur d'avantage). Cela a permis √† l'agent d'apprendre beaucoup plus efficacement.</li>
    <li><strong>Prioritized Experience Replay (PER) :</strong> Au lieu de r√©viser ses exp√©riences au hasard, l'agent avec PER devient un √©tudiant plus intelligent : il passe plus de temps √† √©tudier les cas o√π il s'est le plus tromp√© (les exp√©riences "surprenantes"), ce qui acc√©l√®re son apprentissage.</li>
    <li><strong>Rainbow DQN :</strong> Le summum. Imaginez que vous prenez toutes les meilleures am√©liorations ci-dessus et que vous les combinez en un seul algorithme surpuissant. C'est exactement ce qu'est Rainbow DQN, qui a domin√© tous les pr√©c√©dents sur les jeux Atari.</li>
</ul>

<p>Malgr√© toute cette ing√©niosit√©, toute la famille DQN se heurte √† un mur infranchissable : <strong>elle ne peut pas g√©rer les actions continues</strong>. Pour relever ce d√©fi, il fallait une philosophie compl√®tement diff√©rente. C'est l√† que commence l'histoire des m√©thodes de <em>Policy Gradient</em>.</p>


<h2>Les M√©thodes de Policy Gradient </h2>

<p>Pour piloter un robot, contr√¥ler un bras robotique ou conduire une voiture, il faut une approche capable de g√©rer des actions continues (par exemple, un angle de volant ou une force d'acc√©l√©ration). C'est pr√©cis√©ment l√† qu'interviennent les <strong>m√©thodes de Policy Gradient</strong>.</p>

<p>Ces m√©thodes adoptent une philosophie radicalement diff√©rente de celle du value-based learning (comme DQN). Au lieu d'apprendre une fonction de valeur (Q) pour ensuite en d√©duire la meilleure action, elles apprennent <strong>directement √† optimiser la politique</strong> (le r√©seau de neurones qui prend les d√©cisions).</p>

<h3>Apprentissage "On-Policy" : Apprendre sur le Tas</h3>

<p>Une caract√©ristique essentielle des m√©thodes de Policy Gradient est leur nature <strong>"on-policy"</strong>. Cela signifie qu'elles apprennent directement √† partir des exp√©riences g√©n√©r√©es par la politique actuelle. Contrairement √† DQN, <strong>il n'y a pas de Experience Replay Buffer</strong>. L'agent collecte des trajectoires, les utilise pour mettre √† jour sa politique, puis les jette. C'est un apprentissage plus direct, mais qui peut sembler moins efficient en termes de donn√©es.</p>

<h3>La Fondation : L'Algorithme REINFORCE</h3>

<p>L'algorithme le plus simple et le plus fondamental de cette famille est <strong>REINFORCE</strong>. Son principe est intuitif : il ajuste les poids \(Œ∏\) du r√©seau de neurones de la politique \(œÄ\) pour augmenter la probabilit√© des actions qui ont men√© √† de bonnes r√©compenses.</p>

<p>La mise √† jour s'effectue via une <strong>mont√©e de gradient</strong> (Gradient Ascent) sur un objectif de performance. La formule de mise √† jour est :</p>

$$     \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]
$$
<p>D√©cortiquons cette formule :</p>
<ul>
    <li>\(\nabla_\theta \log \pi_\theta(a_t|s_t)\) : C'est le "score" de la politique. Il indique la direction dans laquelle changer les poids \(Œ∏\) pour <strong>rendre l'action \(a\) plus probable</strong> dans l'√©tat \(s\).</li>
    <li>\(G_t\) : C'est le <strong>retour cumul√©</strong> (la somme des r√©compenses futures escompt√©es) √† partir de l'√©tape `t`.</li>
</ul>

<p>En multipliant le score par le retour \(G_t\), on applique le principe du "carotte et du b√¢ton" :</p>
<ul>
    <li>Si \(G_t\) est √©lev√© (une bonne s√©quence d'actions), on augmente la probabilit√© des actions qui y ont men√©.</li>
    <li>Si \(G_t\) est faible (une mauvaise s√©quence), on diminue la probabilit√© des actions correspondantes.</li>
</ul>

<p><strong>Le Probl√®me Majeur de REINFORCE : La Variance</strong><br>
    Le probl√®me de REINFORCE est sa <strong>variance √©norme</strong>. Le signal \(G_t\) n'est connu qu'√† la fin de l'√©pisode. Une seule r√©compense al√©atoire √† la fin d'une longue trajectoire peut fausser compl√®tement le jugement sur toutes les actions qui l'ont pr√©c√©d√©e, rendant l'apprentissage tr√®s bruit√© et inefficace.</p>

<h3>La Solution : L'Architecture Actor-Critic</h3>

<p>Pour r√©duire cette variance, la solution a √©t√© d'introduire un <strong>Critic</strong>. L'architecture <strong>Actor-Critic</strong> combine le meilleur des deux mondes :</p>
<ul>
    <li><strong>L'Actor</strong> : C'est le r√©seau de neurones de la politique (comme dans REINFORCE) qui choisit les actions. Il est entra√Æn√© par <strong>mont√©e de gradient</strong>.</li>
    <li><strong>Le Critic</strong> : C'est un second r√©seau de neurones qui apprend une fonction de valeur, typiquement \(V(s)\) (la valeur d'un √©tat). Il est entra√Æn√© par <strong>descente de gradient</strong> (comme un r√©seau de r√©gression) pour minimiser l'erreur entre ses pr√©dictions et les retours r√©els.</li>
</ul>

<p>Le Critic fournit √† l'Actor un signal beaucoup plus pr√©cis et moins bruit√© que le retour final \(G_t\). Il ne dit pas juste "c'√©tait bien" ou "c'√©tait mal", mais "<strong>c'√©tait mieux/moins bien que pr√©vu</strong> pour cette situation". Ce signal est appel√© <strong>"Advantage"</strong> (avantage) :</p>

$$     A(s,a) = Q(s,a) - V(s)
$$
<p>L'avantage mesure √† quel point une action est meilleure (ou pire) que la moyenne des actions possibles depuis cet √©tat. La mise √† jour de l'Actor devient alors beaucoup plus stable, en rempla√ßant \(G_t\) par l'avantage \(A(s,a)\) :</p>

$$     \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) A(s_t, a_t) \right]
$$
<p>On ne r√©compense plus une action pour son r√©sultat final, mais parce qu'elle a √©t√© <strong>meilleure que ce √† quoi on s'attendait √† cet instant</strong>.</p>

<p><strong>Le Grand Avantage des Policy Gradients :</strong> Ils peuvent g√©rer nativement les <strong>espaces d'actions continues</strong>. Le r√©seau de neurones peut directement sortir une valeur (par exemple, entre -1 et 1 pour un volant) en utilisant des fonctions d'activation comme \(tanh\), ou une distribution de probabilit√©s sur un intervalle continu.</p>

<h2>Les Champions Modernes : A2C et PPO</h2>

<p>Les m√©thodes Actor-Critic repr√©sentaient une avanc√©e significative, mais des d√©fis subsistaient. L'apprentissage "on-policy" est souvent inefficace en termes d'√©chantillonnage (chaque donn√©e n'est utilis√©e qu'une seule fois) et peut souffrir d'une variance √©lev√©e. De plus, la mise √† jour de la politique elle-m√™me restait un processus d√©licat, o√π une seule mauvaise √©tape pouvait d√©stabiliser tout l'entra√Ænement. C'est pour r√©pondre √† ces probl√®mes que des algorithmes plus sophistiqu√©s comme A2C et PPO ont √©t√© d√©velopp√©s.</p>

<h3>A2C : La Stabilit√© par la Parall√©lisation</h3>

<p>Pour am√©liorer l'efficacit√© et la stabilit√© de l'apprentissage on-policy, la solution a √©t√© de parall√©liser la collecte de donn√©es. L'algorithme <strong>A2C (Advantage Actor-Critic)</strong> est une version synchrone et stable de cette approche.</p>

<p><strong>Le Principe des Multi-Workers :</strong><br>
    Au lieu d'un seul agent interagissant avec un seul environnement, A2C d√©ploie plusieurs agents, appel√©s <strong>"workers"</strong>, qui fonctionnent en parall√®le. Chaque worker poss√®de une copie de la politique actuelle et interagit avec sa propre instance de l'environnement.</p>

<p><strong>Le Processus d'Entra√Ænement :</strong></p>
<ol>
    <li><strong>Collecte Parall√®le :</strong> Tous les workers ex√©cutent la politique simultan√©ment pendant un nombre d√©fini de pas (\(n\)-steps). Chaque worker collecte ainsi une petite trajectoire d'exp√©riences \( (\text{√©tat},\, \text{action},\, \text{r√©compense},\, \text{nouvel √©tat}) \).</li>    <li><strong>Agr√©gation des Donn√©es :</strong> √Ä la fin de cette phase de collecte, toutes les trajectoires de tous les workers sont regroup√©es et envoy√©es √† un agent central.</li>
    <li><strong>Calcul de l'Avantage :</strong> L'agent central utilise cet ensemble de donn√©es pour calculer les estimations d'avantage. Souvent, un m√©canisme appel√© <strong>GAE (Generalized Advantage Estimation)</strong> est utilis√©. Le GAE est une technique sophistiqu√©e qui calcule une estimation de l'avantage en faisant un compromis entre le biais (comme dans le TD(1)) et la variance (comme dans Monte Carlo), en pond√©rant les avantages √† plusieurs pas dans le futur.</li>
    <li><strong>Mise √† Jour Unique :</strong> Une seule mise √† jour agr√©g√©e est effectu√©e sur les r√©seaux centraux (Actor et Critic) en utilisant toutes les donn√©es collect√©es.</li>
    <li><strong>Synchronisation :</strong> Les poids des r√©seaux mis √† jour sont ensuite redistribu√©s √† tous les workers pour la prochaine phase de collecte.</li>
</ol>

<p>
    Cette approche brise la corr√©lation des donn√©es, car les workers explorent des √©tats diff√©rents de mani√®re non s√©quentielle. Elle acc√©l√®re consid√©rablement la collecte d'exp√©riences et r√©duit la variance de l'estimation du gradient, rendant l'entra√Ænement global plus stable et rapide qu'un agent on-policy mono-thread.</p>

<h3>PPO : La Robustesse par la Contrainte</h3>

<p>M√™me avec une collecte de donn√©es am√©lior√©e gr√¢ce √† A2C, le risque des <strong>"destructively large policy updates"</strong> (mises √† jour destructrices de la politique) demeurait. L'objectif de la politique pouvait changer si radicalement d'une mise √† jour √† l'autre que les performances s'effondraient.</p>

<p><strong>Proximal Policy Optimization (PPO)</strong> a √©t√© con√ßu pour r√©soudre ce probl√®me fondamental en introduisant une contrainte directe sur la magnitude des mises √† jour de la politique.</p>

<p>
    L'innovation centrale de PPO est sa fonction objectif "clipp√©e". Au lieu de maximiser directement la performance attendue, PPO maximise un <strong>objectif substitut (surrogate objective)</strong> qui approxime la performance r√©elle tout en p√©nalisant les grands changements de politique.</p>

<p>Cette fonction objectif repose sur plusieurs √©l√©ments cl√©s :</p>

<ol>
    <li><strong>Le Ratio de Probabilit√© :</strong>
        $$             r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
        $$             Ce ratio mesure √† quel point la nouvelle politique \(œÄ_Œ∏\) a chang√© par rapport √† l'ancienne politique \( \pi_{\theta_{\mathrm{old}}} \) pour une action sp√©cifique \(a_t\) qui a √©t√© prise. Un ratio de 1 signifie aucun changement, un ratio sup√©rieur √† 1 signifie que l'action est devenue plus probable, et inversement.</li>
    <li><strong>La Fonction "Clip" :</strong>
        $$             \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)
        $$             Cette fonction contraint le ratio \(r_t(Œ∏)\) √† rester dans un petit intervalle \([1-Œµ, 1+Œµ]\). Si le ratio d√©passe cette borne, il est "coup√©" (clipped) √† la valeur la plus proche de la borne.</li>
    <li><strong>L'Objectif Final :</strong>
        $$             L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
        $$             L'op√©rateur \(min()\) est crucial. Il cr√©e une borne pour l'objectif.
    </li>
</ol>

<p>
    Cette astuce math√©matique force l'algorithme √† faire des mises √† jour de politique petites et prudentes. Il ignore les changements qui seraient trop importants, m√™me s'ils semblent prometteurs sur le lot de donn√©es actuel. Cette robustesse intrins√®que est ce qui a fait de PPO l'algorithme de choix par d√©faut pour une vaste gamme de probl√®mes, y compris des applications complexes comme l'alignement des LLM, o√π la stabilit√© est primordiale.</p>

<hr>

<h2>Aligner les LLMs avec le RLHF</h2>

<p>Et maintenant, comment un algorithme con√ßu pour contr√¥ler un robot finit-il par enseigner la politesse √† un mod√®le de langage ? C'est l√† que l'histoire devient fascinante.</p>

<p>Un LLM, comme GPT, peut √™tre vu comme un agent de RL.</p>
<ul>
    <li><strong>L'√©tat (State)</strong> : C'est la conversation, le prompt que vous lui donnez.</li>
    <li><strong>L'action (Action)</strong> : C'est le prochain mot (ou "token") qu'il g√©n√®re.</li>
    <li><strong>La politique (Policy)</strong> : C'est le r√©seau de neurones du LLM lui-m√™me, qui calcule les probabilit√©s de chaque mot suivant.</li>
</ul>

<p>Le probl√®me est : comment d√©finir une r√©compense ? Est-ce que la r√©ponse "Le ciel est bleu" vaut +10 ou +5 ? C'est impossible √† mod√©liser avec une simple fonction. C'est l√† qu'intervient le <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, la technique qui a rendu des mod√®les comme ChatGPT si utiles.</p>

<p>Le processus se d√©roule en deux temps :</p>

<ol>
    <li><strong>Entra√Æner un Mod√®le de R√©compense :</strong> On demande √† des humains de comparer deux r√©ponses du LLM √† un m√™me prompt et de dire laquelle est la meilleure (plus utile, plus honn√™te, moins nocive, etc.). Avec des milliers de ces comparaisons, on entra√Æne un <em>autre</em> mod√®le de langage, appel√© <strong>Mod√®le de R√©compense</strong>. Le seul but de ce mod√®le est de noter une r√©ponse sur une √©chelle, en imitant les pr√©f√©rences humaines.</li>
    <li><strong>Affiner le LLM avec PPO :</strong> On utilise ensuite PPO pour affiner notre LLM initial.
        <ul>
            <li>L'<strong>Actor</strong> est le LLM que nous voulons aligner.</li>
            <li>L'<strong>Environnement</strong> est l'interaction avec l'utilisateur.</li>
            <li>La <strong>R√©compense</strong> est donn√©e par notre Mod√®le de R√©compense fra√Æchement entra√Æn√© !</li>
        </ul>
    </li>
</ol>

<p>PPO va alors g√©n√©rer des r√©ponses, les faire noter par le mod√®le de r√©compense, et ajuster ses propres poids pour maximiser cette note. Gr√¢ce √† sa nature prudente, il am√©liore le LLM sans le "casser", en l'orientant progressivement vers des comportements que les humains jugent d√©sirables. C'est ainsi que l'on apprend √† une IA √† √™tre plus qu'un simple pr√©dicteur de mots, mais un assistant utile et s√ªr.</p>

<iframe data-tally-src="https://tally.so/embed/3lo6WV?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="299" frameborder="0" marginheight="0" marginwidth="0" title="Newsletter"></iframe>
<script>var d=document,w="https://tally.so/widgets/embed.js",v=function(){"undefined"!=typeof Tally?Tally.loadEmbeds():d.querySelectorAll("iframe[data-tally-src]:not([src])").forEach((function(e){e.src=e.dataset.tallySrc}))};if("undefined"!=typeof Tally)v();else if(d.querySelector('script[src="'+w+'"]')==null){var s=d.createElement("script");s.src=w,s.onload=v,s.onerror=v,d.body.appendChild(s);}</script>

<h2>Conclusion</h2>

<p>Notre voyage th√©orique touche √† sa fin. Vous avez maintenant une compr√©hension compl√®te de l'√©cosyst√®me du Deep Reinforcement Learning :</p>
<ul>
    <li><strong>DQN</strong> pour les probl√®mes √† actions discr√®tes o√π l'efficacit√© d'√©chantillonnage est reine.</li>
    <li><strong>A2C</strong> comme une base solide pour l'apprentissage on-policy parall√©lis√©.</li>
    <li><strong>PPO</strong> comme l'algorithme de r√©f√©rence, robuste et polyvalent, qui alimente certaines des applications IA les plus critiques d'aujourd'hui.</li>
</ul>

<p>Plus important encore, vous comprenez la philosophie derri√®re ces outils : le dilemme exploration-exploitation, le compromis biais-variance, et l'importance de la stabilit√©.</p>

<div class="pack-cta">
    üéÅ <strong>AI Prototype Pack Vol.1</strong> ‚Äî Gratuit, sans carte bancaire.
    <a href="https://openlabs.mychariow.shop/prd_5re50b" target="_blank" rel="noopener">T√©l√©charger</a>
</div>

<h3>Liens Utiles</h3>
<p>Pour approfondir vos connaissances et explorer des outils avanc√©s, voici quelques ressources :</p>
<ul>
    <li><a href="https://www.youtube.com/playlist?list=PLlI0-qAzf2SZ1rdZVFljMWsmZrEvax8bg"> Hors S√©rie - Deep Reinforcement Learning</a> (Playlist) </li>
    <li><a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction">Deep Reinforcement Learning Course</a> de HuggingFace</li>
    <li><a href="https://github.com/deep-learning-indaba/indaba-pracs-2025/tree/main/practicals/Reinforcement_Learning">Reinforcement Learning</a>: Tutoriel DLI25</li>
</ul>

<div class="music-suggestion">
    <p>Ma recommandation musicale du jour : √† √©couter sans mod√©ration !</p>
    <a href="https://www.youtube.com/watch?v=9PukqhfMxfc" target="_blank">√âcouter sur YouTube</a>
</div>



<footer>
    <p><span class="copyleft">&copy;</span> 2025 Jonathan Suru. This work is free.</p>
</footer>
<!-- 100% privacy-first analytics -->
<script data-collect-dnt="true" async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=true" alt="" referrerpolicy="no-referrer-when-downgrade"/></noscript>
</body>
</html>