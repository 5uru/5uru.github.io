<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:image" content="https://jonathansuru.me/thumbnail.png"/>
    <meta property="og:image:alt" content="Code Learn Share" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <title>Introduction au Reinforcement Learning (Partie 1)</title>

    <!-- Polyfill pour compatibilit√© ES6 -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- MathJax pour les formules math√©matiques -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Feuille de style externe -->
    <link rel="stylesheet" href="../styles.css">
</head>
<body>

<header>
    <h1>Jonathan Suru</h1>
    <nav>
        <a href="../index.html">Accueil</a> |
        <a href="../projects.html">Projets</a> |
        <a href="../about.html">√Ä propos</a>
    </nav>
</header>

<main>
    <section>
        <h1>Introduction Au Reinforcement Learning </h1>
        <p>Apr√®s avoir suivi la formation Hors-S√©rie sur le Deep Reinforcement Learning propos√©e par FIDLE, j'ai d√©cid√© de compiler et de structurer ici les concepts fondamentaux de l'apprentissage par renforcement. L'objectif de cet article est de pr√©senter de mani√®re claire et ordonn√©e les bases th√©oriques et les premiers algorithmes qui constituent le socle de cette discipline.</p>
    </section>
    <section>
        <h3>Qu'est-ce que l'Apprentissage par Renforcement ?</h3>
        <p>L'<strong>Apprentissage par Renforcement</strong> (Reinforcement Learning, RL) est un paradigme d'apprentissage automatique o√π un agent apprend √† prendre des d√©cisions en interagissant avec un environnement. L'objectif est de maximiser une r√©compense cumul√©e au fil du temps. Ce processus est souvent r√©sum√© par la maxime : "You Win or You Learn!" (Soit vous gagnez, soit vous apprenez).</p>
        <p>Imaginez un √©cureuil qui se d√©place sur une grille. √Ä chaque pas, elle doit choisir une action : aller √† gauche, √† droite, en haut ou en bas. Si elle atteint une case contenant de la nourriture, elle re√ßoit une r√©compense. Si elle tombe sur la case du loup, elle subit une p√©nalit√©. Sans carte ni instructions, l‚Äô√©cureuil apprend progressivement, par essais et erreurs, √† √©viter le danger et √† maximiser ses chances de trouver de la nourriture. C‚Äôest exactement ce que fait un agent en apprentissage par renforcement : il affine sa politique en fonction des r√©compenses et des punitions re√ßues lors de ses interactions avec l‚Äôenvironnement.</p>    </section>

    <section>
        <h3>Vocabulaire Fondamental du RL</h3>
        <p>Pour comprendre le RL, il est crucial de ma√Ætriser son vocabulaire. Ces termes ne sont pas ind√©pendants ; ils d√©crivent les diff√©rentes facettes d'une boucle d'apprentissage continue. L'agent observe son <strong>√©tat</strong> actuel dans l'<strong>environnement</strong>, choisit une <strong>action</strong> √† effectuer, et re√ßoit en retour une <strong>r√©compense</strong> et un nouvel √©tat. Sa strat√©gie pour choisir les actions est sa <strong>politique</strong>. L'objectif ultime est de trouver la politique qui maximise le <strong>retour</strong> total sur le long terme.</p>
        <p>Voici les briques de base de tout probl√®me de RL :</p>
        <ul>
            <li><strong>Agent</strong> : Le preneur de d√©cision (le programme, le robot). Il observe et agit.</li>
            <li><strong>Environnement</strong> : Le monde avec lequel l'agent interagit. Il fournit des √©tats et des r√©compenses √† l'agent.</li>
            <li><strong>√âtat (State, not√© s)</strong> : Une repr√©sentation compl√®te et instantan√©e de la situation de l'agent dans l'environnement.</li>
            <li><strong>Action (Action, not√©e a)</strong> : Un choix ou un mouvement que l'agent peut effectuer. L'ensemble des actions possibles depuis un √©tat est appel√© <strong>l'espace d'actions</strong>.</li>
            <li><strong>R√©compense (Reward, not√©e R)</strong> : Un signal scalaire envoy√© par l'environnement √† l'agent apr√®s chaque action. Une r√©compense positive indique que l'action a √©t√© b√©n√©fique, une p√©nalit√© (r√©compense n√©gative) qu'elle a √©t√© n√©faste. La fonction de r√©compense est le guide principal de l'agent.</li>
            <li><strong>Politique (Policy, not√©e œÄ)</strong> : La strat√©gie de l'agent. C'est une fonction qui associe un √©tat √† une action (ou √† une distribution de probabilit√©s sur les actions). L'objectif du RL est de trouver la politique optimale \( \pi^{*} \).</li>
            <li><strong>Retour (Return, not√© G‚Çú)</strong> : La r√©compense totale cumul√©e future, souvent escompt√©e par un facteur <code>Œ≥</code>. C'est la quantit√© que l'agent cherche √† maximiser.
                <blockquote>$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... $$</blockquote>
            </li>
        </ul>
    </section>

    <section>
        <h3>Le Cadre Th√©orique : Le Contr√¥le Optimal et les √âquations de Bellman</h3>
        <p>Le <strong>Contr√¥le Optimal</strong> est une branche des math√©matiques qui s'int√©resse au probl√®me de trouver une commande ou une politique qui optimise un certain crit√®re de performance. Dans le cadre de l'apprentissage par renforcement, cela se traduit par une situation id√©ale o√π l'agent a une connaissance parfaite de son environnement.</p>
        <p>Cela signifie qu'il conna√Æt deux choses fondamentales : les probabilit√©s de transition d'un √©tat √† un autre pour chaque action (\(P(s' \mid s, a)\)) et la fonction de r√©compense (\(R(s, a)\)). Contrairement √† l'apprentissage par renforcement o√π l'agent doit d√©couvrir ces r√®gles par essais et erreurs, le contr√¥le optimal permet de <strong>calculer</strong> directement la meilleure strat√©gie possible. C'est donc un probl√®me de <strong>planification</strong> et non d'apprentissage. Ce mod√®le th√©orique est crucial car il d√©finit la solution parfaite, l'objectif vers lequel les algorithmes d'apprentissage vont tendre √† s'approcher.</p>
        <p>Pour mod√©liser un probl√®me de RL, on utilise le <strong>Processus de D√©cision Markovien (MDP)</strong>. Un MDP est un cadre math√©matique qui d√©crit un environnement o√π les cons√©quences des actions sont partiellement al√©atoires et o√π l'√©tat futur ne d√©pend que de l'√©tat pr√©sent et de l'action choisie (propri√©t√© de Markov).</p>

        <h4>Les Fonctions de Valeur</h4>
        <p>Pour √©valuer la qualit√© d'un √©tat ou d'une action, on utilise des fonctions de valeur. Ce sont des outils de pr√©diction qui nous disent √† quoi s'attendre dans le futur.</p>
        <ul>
            <li><strong>V(s) - La fonction de valeur d'√©tat</strong> : Repr√©sente le retour attendu si l'agent commence √† l'√©tat <em>s</em> et suit ensuite une politique donn√©e. Elle r√©pond √† la question : "√Ä quel point cet √©tat est-il bon ?".</li>
            <li><strong>\(Q(S, A)\) - La fonction de valeur d'action-√©tat</strong> : Repr√©sente le retour attendu si l'agent est √† l'√©tat <em>s</em>, effectue l'action <em>a</em>, puis suit la politique. Elle r√©pond √† la question : "√Ä quel point est-ce une bonne id√©e de faire cette action ici ?". C'est souvent la fonction la plus utile en pratique.</li>
        </ul>

        <h4>Les √âquations de Bellman</h4>
        <p>La r√©volution th√©orique vient avec les <strong>√âquations de Bellman</strong>. Elles expriment une id√©e intuitive mais puissante : la valeur d'une situation est √©gale √† la r√©compense imm√©diate que l'on obtient, plus la valeur de la situation future dans laquelle on se retrouve. C'est une relation de r√©currence qui permet de calculer la valeur de n'importe quel √©tat ou action.</p>
        <p><strong>√âquation de Bellman pour V(s) :</strong></p>
        <blockquote>$$ V(s) = \max_{a}  \left( R(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s') \right) $$</blockquote>
        <p>Dans cette √©quation, le terme <strong>Œ≥ (gamma)</strong> est le <strong>taux d'escompte</strong>. C'est un nombre compris entre 0 et 1 qui d√©termine l'importance que l'accorde aux r√©compenses futures par rapport aux r√©compenses imm√©diates. Un <code>Œ≥</code> proche de 1 rend l'agent tr√®s patient, car il valorise fortement les r√©compenses lointaines. √Ä l'inverse, un <code>Œ≥</code> proche de 0 rend l'agent "avide", car il se concentre presque exclusivement sur les r√©compenses imm√©diates. Le choix de <code>Œ≥</code> est donc crucial pour d√©finir le comportement de l'agent.</p>
        <p><strong>√âquation de Bellman pour \(Q(S, A)\) :</strong></p>
        <blockquote>$$ Q(S, A) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q(s',a') $$</blockquote>
        <p>Pour une politique optimale \( \pi^{*} \), la relation entre V et Q est simple : \( V^{*}(s) = \max_{a} Q^{*}(s,a) \).</p>    </section>

    <section>
        <h3>L'Apprentissage dans un Environnement Inconnu</h3>
        <p>Le monde du contr√¥le optimal est un id√©al. Dans la r√©alit√©, l'agent ne conna√Æt pas les r√®gles du jeu. Il ne sait pas quelle r√©compense il obtiendra ni o√π il atterrira apr√®s une action. Il doit tout apprendre par lui-m√™me, en interagissant. C'est l√† que le v√©ritable <strong>Reinforcement Learning</strong> commence.</p>

        <h4>Le Dilemme Exploration vs. Exploitation</h4>
        <p>Cette situation cr√©e un dilemme fondamental pour l'agent. Doit-il <strong>exploiter</strong> ses connaissances actuelles en choisissant l'action qui lui a rapport√© le plus de r√©compenses jusqu'√† pr√©sent ? Ou doit-il <strong>explorer</strong> en essayant de nouvelles actions qui pourraient s'av√©rer encore meilleures, mais qui pourraient aussi √™tre d√©sastreuses ?</p>
        <p>C'est comme choisir un restaurant. L'exploitation, c'est aller √† votre restaurant pr√©f√©r√©, vous √™tes s√ªr d'y passer un bon moment. L'exploration, c'est essayer ce nouveau restaurant qui vient d'ouvrir. Il pourrait √™tre g√©nial, ou d√©cevant. Pour maximiser votre satisfaction sur le long terme, vous devez trouver le bon √©quilibre.</p>
        <p>Une strat√©gie courante est la <strong>politique Œµ-greedy</strong> : avec une probabilit√© <code>Œµ</code>, l'agent explore (action al√©atoire), et avec une probabilit√© <code>1-Œµ</code>, il exploite (meilleure action connue).</p>

        <h4>M√©thodes d'Apprentissage</h4>
        <p>Pour estimer les fonctions de valeur sans conna√Ætre l'environnement, l'agent doit apprendre de son exp√©rience. Mais comment ? Faut-il attendre la fin d'une t√¢che pour juger, ou peut-on apprendre √† chaque instant ? Plusieurs approches existent, chacune avec sa propre "personnalit√©".</p>

        <h4>1. M√©thode Monte Carlo</h4>
        <p>La m√©thode Monte Carlo est celle de l'historien patient. Elle attend la fin d'un √©pisode (un "game over") pour juger de la valeur des actions. Elle regarde la s√©quence compl√®te des r√©compenses (le retour <code>G‚Çú</code> r√©el) et attribue cette valeur √† chaque √©tat-action qui a √©t√© visit√©.</p>
        <p><strong>√âtapes :</strong></p>
        <ol>
            <li>Initialiser la table \(Q(S, A)\).</li>
            <li>Pour chaque √©pisode :
                <ol type="a">
                    <li>G√©n√©rer un √©pisode complet en suivant une politique (ex: Œµ-greedy), en stockant chaque transition \( (S_t, A_t, R_{t+1}) \).</li>
                    <li>Lorsque l'√©pisode se termine √† l'√©tape T, calculer le retour \( G_t \) pour chaque √©tape <code>t</code>.</li>
                    <li>Pour chaque paire √©tat-action  \( (S_t, A_t) \) visit√©e, mettre √† jour \( Q(S_t, A_t) \) en fonction du retour \( G_t \).</li>
                </ol>
            </li>
        </ol>
        <p>Elle est <strong>sans biais</strong> (car bas√©e sur des retours r√©els) mais √† <strong>forte variance</strong> (un seul √©pisode chanceux ou malchanceux peut fausser l'estimation).</p>

        <h4>2. Apprentissage par Diff√©rence Temporelle (TD)</h4>
        <p>La m√©thode TD est celle du r√©aliste impatient. Elle n'attend pas la fin de l'√©pisode. Elle met √† jour ses estimations √† chaque √©tape, en utilisant une estimation pour la r√©compense future. C'est ce qu'on appelle le <strong>bootstrap</strong> : on met √† jour une estimation avec une autre estimation.</p>
        <p><strong>√âtapes :</strong></p>
        <ol>
            <li>Initialiser la table \(Q(S, A)\).</li>
            <li>Pour chaque √©tape :
                <ol type="a">
                    <li>Observer l'√©tat \(S\), choisir une action \(A\) (Œµ-greedy).</li>
                    <li>Ex√©cuter \(A\), observer \(R\) et \(S'\).</li>
                    <li>Mettre √† jour : \(Q(S, A) \leftarrow Q(S, A) + \alpha \cdot [R + \gamma \cdot Q(S', A') - Q(S, A)]\).</li>
                    <li>Mettre √† jour l'√©tat : \(S \leftarrow S'\).</li>
                </ol>
            </li>
        </ol>
        <p>Elle est <strong>biais√©e</strong> (car l'estimation \(Q(S', A')\) peut √™tre fausse) mais √† <strong>faible variance</strong>, ce qui la rend beaucoup plus stable et rapide √† apprendre.</p>

        <h4>3. Apprentissage N-step Temporal Difference : Le Juste Milieu</h4>
        <p>Si Monte Carlo est trop patient et TD est trop impatient, l'apprentissage <strong>N-step TD</strong> offre un compromis intelligent. Au lieu de regarder seulement 1 pas dans le futur ou d'attendre la fin de l'√©pisode, il regarde <code>n</code> pas en avant.</p>
        <p>Pensez √† un analyste financier. L'approche 1-step TD ne regarde que le prix d'une action demain pour pr√©dire sa valeur aujourd'hui. L'approche Monte Carlo attend la fin du mois pour voir le profit total. L'approche N-step TD (avec n=5, par exemple) regarde l'√©volution des prix sur les 5 prochains jours avant de faire une pr√©diction. C'est plus informatif qu'un seul jour, mais plus rapide que d'attendre la fin du mois.</p>
        <p>La <strong>cible N-step TD</strong> est calcul√©e en additionnant les <code>n-1</code> premi√®res r√©compenses r√©elles, puis en ajoutant une estimation de la valeur √† l'√©tape <code>n</code>.</p>
        <blockquote>$$ Cible\_TD = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n Q(s_{t+n}, a_{t+n}) $$</blockquote>
        <p><strong>√âtapes :</strong></p>
        <ol>
            <li>Initialiser la table \(Q(S, A)\).</li>
            <li>Pour chaque √©tape :
                <ol type="a">
                    <li>Observer l'√©tat \(S\), choisir une action \(A\) (Œµ-greedy).</li>
                    <li>Ex√©cuter \(A\), observer \(R\) et \(S'\).</li>
                    <li>Stocker la transition \((S, A, R, S')\) dans un buffer temporaire.</li>
                    <li>Si \(n\) √©tapes se sont √©coul√©es depuis la transition \((S_t, A_t)\) stock√©e :
                        <ol type="i">
                            <li>Calculer la cible N-step TD en utilisant les \(n\) r√©compenses et l'√©tat \(S_{t+n}\).</li>
                            <li>Mettre √† jour \(Q(S_t, A_t)\) avec cette cible.</li>
                        </ol>
                    </li>
                </ol>
            </li>
        </ol>
        <p>La beaut√© de N-step TD est qu'il permet de contr√¥ler le compromis biais-variance. Plus \(n\) est grand, plus on se rapproche de Monte Carlo (moins de biais, plus de variance). Plus \(n\) est petit, plus on se rapproche de TD (plus de biais, moins de variance). En choisissant \(n\) on peut trouver l'√©quilibre parfait pour un probl√®me donn√©.</p>
    </section>

    <section>
        <h3>Les Algorithmes d'Apprentissage : SARSA et Q-Learning</h3>
        <p>Les m√©thodes TD nous donnent un principe de mise √† jour. SARSA et Q-Learning sont deux algorithmes concrets qui appliquent ce principe. Ils sont tous deux extr√™mement populaires, mais ils diff√®rent fondamentalement dans leur mani√®re d'apprendre, une distinction cruciale entre <em>on-policy</em> et <em>off-policy</em>.</p>

        <h3>SARSA (On-Policy) : L'Apprenti Prudent</h3>
        <p>SARSA est un algorithme <strong>on-policy</strong> : il apprend la valeur de la politique qu'il est en train d'ex√©cuter, avec toutes ses imperfections et ses phases d'exploration. Sa cible TD utilise l'action \(a'\) que l'agent a <strong>r√©ellement</strong> choisie dans l'√©tat suivant \(s'\).</p>
        <blockquote>$$ Q(S, A) \leftarrow Q(S, A) + \alpha \times [R + \gamma Q(s', a') - Q(S, A)] $$</blockquote>
        <p><strong>√âtapes :</strong></p>
        <ol>
            <li>Initialiser \(Q\).</li>
            <li>Pour chaque √©pisode :
                <ol type="a">
                    <li>Initialiser \(S\), choisir \(A\) (Œµ-greedy).</li>
                    <li>R√©p√©ter :
                        <ol type="i">
                            <li>Ex√©cuter \(A\), observer \(R\) et \(S'\).</li>
                            <li>Choisir \(A'\) depuis \(S'\) (Œµ-greedy).</li>
                            <li>Mettre √† jour \(Q(S, A)\) avec la formule ci-dessus.</li>
                            <li>\(S \leftarrow S'\), \(A \leftarrow A'\).</li>
                        </ol>
                    </li>
                </ol>
            </li>
        </ol>
        <p>SARSA est <strong>prudent</strong>. Il apprend √† naviguer en tenant compte du fait qu'il peut faire des erreurs d'exploration.</p>

        <h3>Q-Learning (Off-Policy) : Le Visionnaire Audacieux</h3>
        <p>Q-Learning est un algorithme <strong>off-policy</strong> : il apprend la valeur de la politique optimale tout en suivant une politique exploratoire. Sa cible TD utilise la meilleure action possible depuis l'√©tat <code>s'</code>, quelle que soit l'action qu'il a r√©ellement choisie.</p>
        <blockquote>$$ Q(S, A) \leftarrow Q(S, A) + \alpha \times [R + \gamma \times \max_{a'} Q(s', a') - Q(S, A)] $$</blockquote>
        <p><strong>Algorithme (√©tapes) :</strong></p>
        <ol>
            <li>Initialiser \(Q\).</li>
            <li>Pour chaque √©pisode :
                <ol type="a">
                    <li>Initialiser \(S\).</li>
                    <li>R√©p√©ter :
                        <ol type="i">
                            <li>Choisir \(A\) depuis \(S\) (Œµ-greedy).</li>
                            <li>Ex√©cuter \(A\), observer \(R\) et \(S'\).</li>
                            <li>Mettre √† jour \(Q(S, A)\) avec la formule ci-dessus.</li>
                            <li>\(S \leftarrow S'\).</li>
                        </ol>
                    </li>
                </ol>
            </li>
        </ol>
        <p>Q-Learning est plus <strong>audacieux</strong>. Il apprend le chemin optimal en supposant qu'il finira par toujours choisir les meilleures actions. Sa nature <em>off-policy</em> le rend aussi tr√®s efficace, car il peut apprendre de n'importe quelle exp√©rience pass√©e.</p>
    </section>

    <section>
        <h3>Limites de l'Approche Tabulaire et Perspective</h3>
        <p>Les m√©thodes que nous avons vues sont incroyablement puissantes, mais elles ont un talon d'Achille : leur m√©moire. Elles sont dites <strong>tabulaires</strong>, car elles stockent les valeurs Q dans une table. Cette approche fonctionne parfaitement pour des mondes simples comme un labyrinthe, mais elle se heurte √† un mur infranchissable d√®s que l'on s'attaque √† des probl√®mes r√©els.</p>
        <ul>
            <li><strong>La Mal√©diction de la Dimensionnalit√©</strong> : Le nombre d'√©tats possibles explose pour des environnements r√©els. Si l'√©tat de votre agent est une image de 84x84 pixels, le nombre d'√©tats possibles est sup√©rieur au nombre d'atomes dans l'univers. Une table Q devient impossible √† stocker.</li>
            <li><strong>Incapacit√© √† G√©n√©raliser</strong> : Un agent tabulaire est un "idiot-savant". Il ne conna√Æt que ce qu'il a litt√©ralement exp√©riment√©. S'il apprend qu'une position est bonne, il n'a aucune id√©e qu'une position presque identique est probablement aussi bonne. Il ne transf√®re pas son savoir.</li>
            <li><strong>Espaces d'Actions Continus</strong> : Comment g√©rer une action comme "tourner le volant de 15.7 degr√©s" ? Avec une table, vous √™tes oblig√© de <strong>discr√©tiser</strong>, ce qui est une approximation grossi√®re.</li>
        </ul>

        <h3>Le Deep Reinforcement Learning</h3>
        <p>Pour surmonter ces limites, le <strong>Deep Reinforcement Learning</strong> a marqu√© un tournant majeur. L'id√©e est g√©niale dans sa simplicit√© : remplacer la table Q rigide par un r√©seau de neurones profond, une fonction souple capable d'<strong>approximer</strong> la valeur Q pour n'importe quel √©tat. Le r√©seau de neurones apprend √† g√©n√©raliser √† partir des exemples qu'il voit, lui permettant d'√©valuer des situations nouvelles mais similaires.</p>
    </section>

    <section>
        <h3>Conclusion</h3>
        <p>Dans cet article, nous avons √©tabli les fondations de l'apprentissage par renforcement. Nous sommes partis du cadre th√©orique id√©al du contr√¥le optimal et des √©quations de Bellman, qui d√©crivent comment raisonner sur la valeur √† long terme si l'environnement est parfaitement connu.</p>
        <p>Nous avons ensuite bris√© cette hypoth√®se pour entrer dans le v√©ritable apprentissage, o√π un agent doit d√©couvrir la politique optimale par lui-m√™me. Nous avons vu comment il doit g√©rer le dilemme exploration/exploitation et comment des m√©thodes comme Monte Carlo et les diff√©rences temporelles lui permettent d'apprendre de son exp√©rience.</p>
        <p>Enfin, nous avons d√©taill√© les deux algorithmes pionniers, SARSA et Q-Learning, en soulignant la distinction cruciale entre les approches <em>on-policy</em> (prudente) et <em>off-policy</em> (audacieuse et efficace).</p>
        <p>Cependant, nous avons √©galement mis en lumi√®re les limites de ces m√©thodes tabulaires, qui sont inadapt√©es aux probl√®mes complexes du monde r√©el √† cause de la mal√©diction de la dimensionnalit√© et de leur incapacit√© √† g√©n√©raliser.</p>
        <p>C'est pr√©cis√©ment pour surmonter ces obstacles que le <strong>Deep Reinforcement Learning</strong> a √©t√© con√ßu. En rempla√ßant la table rigide par un r√©seau de neurones souple, il a ouvert la voie √† des applications autrefois inimaginables. <strong>Dans un prochain article, nous explorerons cette r√©volution et des algorithmes comme le Deep Q-Network (DQN).</strong></p>

        <div class="pack-cta">
            üéÅ <strong>AI Prototype Pack Vol.1</strong> ‚Äî Gratuit, sans carte bancaire.
            <a href="https://openlabs.mychariow.shop/prd_5re50b" target="_blank" rel="noopener">T√©l√©charger</a>
        </div>

        <h3>Liens Utiles</h3>
        <p>Pour approfondir vos connaissances et explorer des outils avanc√©s, voici quelques ressources :</p>
        <ul>
            <li><a href="https://www.youtube.com/playlist?list=PLlI0-qAzf2SZ1rdZVFljMWsmZrEvax8bg"> Hors S√©rie - Deep Reinforcement Learning</a> (Playlist) </li>
            <li><a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction">Deep Reinforcement Learning Course</a> de HuggingFace</li>
            <li><a href="https://github.com/deep-learning-indaba/indaba-pracs-2025/tree/main/practicals/Reinforcement_Learning">Reinforcement Learning</a>: Tutoriel DLI25</li>
        </ul>

        <div class="music-suggestion">
            <p>Ma recommandation musicale du jour : √† √©couter sans mod√©ration !</p>
            <a href="https://www.youtube.com/watch?v=9PukqhfMxfc" target="_blank">√âcouter sur YouTube</a>
        </div>
    </section>
</main>



<footer>
    <p><span class="copyleft">&copy;</span> 2025 Jonathan Suru. This work is free.</p>
</footer>
<!-- 100% privacy-first analytics -->
<script data-collect-dnt="true" async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=true" alt="" referrerpolicy="no-referrer-when-downgrade"/></noscript>
</body>
</html>