<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Jonathan Suru">
  <title>Knowledge Distillation : L'Art de Compresser l'Intelligence (Partie 1)</title>

  <!-- MathJax pour les formules math√©matiques -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Feuille de style externe -->
  <link rel="stylesheet" href="../styles.css">
  <meta property="og:image" content="https://jonathansuru.me/thumbnail.png"/>
  <meta property="og:image:alt" content="Code Learn Share" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" /><!-- Feuille de style externe -->
  <link rel="stylesheet" href="../styles.css">
  <script>
    window.TallyConfig = {
      "formId": "3lo6WV",
      "popup": {
        "emoji": {
          "text": "üëã",
          "animation": "wave"
        },
        "open": {
          "trigger": "scroll",
          "scrollPercent": 40
        }
      }
    };
  </script>

  <script async src="https://tally.so/widgets/embed.js"></script>
</head>
<body>
<header>
  <h1>Jonathan Suru</h1>
  <nav>
    <a href="../index.html">Accueil</a> |
    <a href="../projects.html">Projets</a> |
    <a href="../about.html">√Ä propos</a>
  </nav>
</header>

<h2>Knowledge Distillation : L'Art de Compresser l'Intelligence (Partie 1)</h2>

<p>Dans le paysage actuel de l'intelligence artificielle, une tension structurelle persiste entre deux imp√©ratifs : la <strong>performance</strong> et l'<strong>efficacit√©</strong>. D'un c√¥t√©, la course aux mod√®les toujours plus gigantesques tels que les LLMs. De l'autre, la r√©alit√© industrielle impose des contraintes drastiques : d√©ploiement sur appareils mobiles, objets connect√©s (IoT), latence r√©duite et n√©cessit√© de minimiser l'empreinte √©nerg√©tique.</p>

<p>J'ai moi-m√™me √©t√© confront√© √† ce dilemme en d√©veloppant mon projet <a href="https://jonathansuru.me/articles/healthbox.html">Healthbox</a>. En travaillant sur cette solution, j'ai rapidement r√©alis√© que la puissance brute d'un mod√®le ne suffisait pas. Pour garantir une exp√©rience utilisateur fluide et r√©active, tout en pr√©servant les ressources mat√©rielles, il √©tait imp√©ratif de r√©duire la taille des mod√®les sans en sacrifier l'efficacit√©. C'est en cherchant une solution √† ce probl√®me pr√©cis que j'ai d√©couvert la puissance de la <strong>Distillation de Connaissances</strong> (<em>Knowledge Distillation</em>).</p>

<em>Cet article, premier volet d'une s√©rie inspir√©e des ateliers pratiques <a href="https://github.com/MENA-ML/lab_sessions_2026/tree/main/distillation">MENA-ML</a>, pose les fondations th√©oriques de cette m√©thode.</em>

<h2>Le Contexte : Les Techniques d'Optimisation</h2>
<p>Avant de plonger dans la distillation, il est essentiel de la situer parmi les autres strat√©gies d'optimisation. R√©duire un mod√®le pour le rendre <strong>plus l√©ger et plus rapide</strong> peut se faire selon plusieurs approches, mais toutes n'ont pas le m√™me impact sur la capacit√© d'apprentissage :</p>
<ul>
  <li><strong>Le Fine-tuning :</strong> Il s'agit d'adapter un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique. Bien qu'utile pour la pr√©cision, il ne r√©duit pas intrins√®quement l'empreinte m√©moire ni la complexit√© de calcul. C'est une adaptation fonctionnelle, pas une compression structurelle.</li>
  <li><strong>La Quantification (Quantization) :</strong> Technique tr√®s r√©pandue consistant √† r√©duire la pr√©cision des nombres stock√©s (ex: passer de flottants 32 bits √† des entiers 8 bit). Elle all√®ge le mod√®le "physiquement" sans changer son architecture, bien qu'elle puisse n√©cessiter un r√©-entra√Ænement pour compenser la perte de pr√©cision.</li>
  <li><strong>Le Pruning (√âlagage) :</strong> Cette m√©thode retire les connexions (poids) ou les neurones les moins pertinents, "taillant" le r√©seau pour le rendre plus l√©ger.</li>
</ul>
<p>La <strong>Distillation de Connaissances</strong> offre une approche radicalement diff√©rente : au lieu de modifier un mod√®le existant, elle propose d'entra√Æner un <strong>nouveau mod√®le compact</strong> (l'√©l√®ve) en le guidant par un mod√®le expert (le ma√Ætre). C'est une m√©thode de transfert d'apprentissage.</p>



<h2>Le Paradigme Ma√Ætre-√âl√®ve</h2>
<p>L'id√©e centrale repose sur une analogie p√©dagogique : le couple <strong>Ma√Ætre-√âl√®ve</strong> (<em>Teacher-Student</em>).</p>
<ul>
  <li><strong>Le "Teacher" (Le Ma√Ætre) :</strong> Un mod√®le pr√©-entra√Æn√©, souvent massif (ou un "ensemble" de mod√®les), qui a appris √† g√©n√©raliser sur de vastes jeux de donn√©es. Il est pr√©cis mais lourd.</li>
  <li><strong>Le "Student" (L'√âl√®ve) :</strong> Un r√©seau de neurones compact, con√ßu pour √™tre rapide et l√©ger. Il a une capacit√© de repr√©sentation moindre.</li>
</ul>
<p>L'objectif n'est pas que l'√©l√®ve se contente d'apprendre les r√©ponses par c≈ìur, mais qu'il apprenne <strong>√† raisonner comme le professeur</strong>. En imitant la mani√®re dont le professeur classe les donn√©es, l'√©l√®ve acquiert une capacit√© de g√©n√©ralisation bien sup√©rieure √† celle qu'il aurait obtenue en apprenant seul.</p>

<h2>Architecture des Acteurs : Comment sont-ils construits ?</h2>
<p>La r√©ussite de la distillation repose sur le choix judicieux de l'architecture de chacun des acteurs. Il ne s'agit pas simplement de prendre deux mod√®les au hasard, mais d'optimiser la relation entre une capacit√© de stockage (Teacher) et une capacit√© de g√©n√©ralisation (Student).</p>

<h3>Le Teacher : Le G√©ant Gel√©</h3>
<p>Le mod√®le Ma√Ætre est g√©n√©ralement un r√©seau de neurones profond et complexe, d√©j√† entra√Æn√© (pr√©-entra√Æn√©) sur de vastes corpus de donn√©es. Sa construction vise la performance brute avant tout :</p>
<ul>
  <li><strong>Architecture :</strong> On utilise souvent des mod√®les √©tat de l'art (SOTA) comme <strong>BERT-Large</strong>, <strong>ResNet-101</strong> ou des <strong>Ensembles</strong> de mod√®les (combinaison de plusieurs r√©seaux). L'id√©e est de maximiser la richesse s√©mantique.</li>
  <li><strong>√âtat pendant la distillation :</strong> Crucialement, les poids du Teacher sont <strong>gel√©s</strong> (<em>frozen</em>). Il ne r√©apprend pas. On l'utilise uniquement en mode inf√©rence pour g√©n√©rer les "Soft Labels" sur les donn√©es d'entra√Ænement. C'est une source de connaissance statique et in√©branlable.</li>
</ul>

<h3>Le Student : L'Architecture Compacte</h3>
<p>La conception de l'√©l√®ve est un exercice d'√©quilibre. Il doit √™tre assez petit pour √™tre rapide, mais assez expressif pour capturer la connaissance du ma√Ætre. Il existe deux strat√©gies principales pour le construire :</p>
<ul>
  <li><strong>Conception "Sur Mesure" (Custom) :</strong> On opte pour des architectures con√ßues pour l'efficacit√©, comme <strong>MobileNet</strong> pour la vision ou <strong>DistilBERT</strong>/TinyBERT pour le NLP. Ces r√©seaux utilisent des op√©rations optimis√©es (convolutions depthwise, factorisation de matrices) pour r√©duire les calculs.</li>
  <li><strong>Simplification du Ma√Ætre :</strong> On prend l'architecture du Teacher et on retire des couches (ex: passer de 12 couches √† 6) ou on r√©duit la largeur des couches (nombre de neurones). C'est l'approche utilis√©e par DistilBERT : m√™me architecture, mais moiti√© moins de couches.</li>
</ul>

<p>Bien que les architectures diff√®rent, la couche de sortie finale doit imp√©rativement avoir la <strong>m√™me dimension</strong> (le m√™me nombre de classes ou de tokens) pour que la comparaison des probabilit√©s (Softmax) entre le Teacher et le Student soit math√©matiquement possible.</p>

<h2>La "Dark Knowledge" : L'Information Cach√©e</h2>
<p>Pour comprendre l'efficacit√© de la distillation, il faut s'int√©resser au concept de <strong>"Dark Knowledge"</strong> (Connaissance Obscure), introduit par Geoffrey Hinton et ses coll√®gues dans leur <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener noreferrer">article fondateur de 2015</a>.</p>

<p>Dans un entra√Ænement classique supervis√©, le mod√®le re√ßoit une v√©rit√© "dure" (<em>Hard Target</em>). Pour une image de chien, le label est : \( [ Chien¬†: 1, Chat¬†: 0, Voiture¬†: 0] \). Cette approche binaire ignore totalement les nuances. Elle dit au mod√®le que le chien n'a rien √† voir avec un chat ou une voiture, ce qui est faux s√©mantiquement : un chien ressemble structurellement plus √† un chat qu'√† une voiture.</p>

<p>Hinton explique que les mod√®les entra√Æn√©s produisent des probabilit√©s pour les classes incorrectes qui ne sont pas al√©atoires. La sortie du Teacher est beaucoup plus riche. Pour le m√™me chien, il pourrait pr√©dire :</p>


<div >
  <!-- Graphique SVG : Comparaison Hard vs Soft Labels -->
  <svg viewBox="0 0 500 350" xmlns="http://www.w3.org/2000/svg" style="font-family: system-ui, -apple-system, sans-serif; max-width: 100%; height: auto; margin: 2em 0;">

    <defs>
      <!-- D√©grad√©s -->
      <linearGradient id="hardGrad" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%" style="stop-color:#94a3b8" />
        <stop offset="100%" style="stop-color:#cbd5e1" />
      </linearGradient>
      <linearGradient id="softMainGrad" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%" style="stop-color:#2563eb" />
        <stop offset="100%" style="stop-color:#3b82f6" />
      </linearGradient>
      <linearGradient id="softDarkGrad" x1="0%" y1="0%" x2="100%" y2="0%">
        <stop offset="0%" style="stop-color:#d97706" />
        <stop offset="100%" style="stop-color:#f59e0b" />
      </linearGradient>
      <!-- Ombre douce -->
      <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
        <feDropShadow dx="0" dy="2" stdDeviation="3" flood-opacity="0.1"/>
      </filter>
    </defs>

    <!-- Fond de carte -->
    <rect x="0" y="0" width="500" height="350" rx="12" fill="#ffffff" filter="url(#shadow)" stroke="#e2e8f0" stroke-width="1"/>

    <!-- TITRE PRINCIPAL -->
    <text x="250" y="35" text-anchor="middle" font-size="18" font-weight="700" fill="#0f172a">Hard Labels vs Soft Labels : La Nuance qui change tout</text>

    <!-- SECTION 1 : HARD LABELS (En haut) -->
    <g transform="translate(20, 60)">
      <rect x="0" y="0" width="460" height="100" rx="8" fill="#f8fafc" stroke="#e2e8f0"/>
      <text x="15" y="25" font-size="13" font-weight="600" fill="#475569">Entra√Ænement Classique (Hard Target)</text>

      <!-- Barres -->
      <g transform="translate(15, 45)">
        <text x="0" y="10" font-size="12" fill="#64748b">Chien</text>
        <rect x="60" y="0" width="350" height="16" rx="3" fill="url(#hardGrad)" />
        <text x="420" y="11" font-size="11" fill="#64748b">100%</text>
      </g>
      <g transform="translate(15, 70)">
        <text x="0" y="10" font-size="12" fill="#64748b">Chat</text>
        <rect x="60" y="0" width="0" height="16" rx="3" fill="#e2e8f0" />
        <text x="65" y="11" font-size="11" fill="#94a3b8">0%</text>
      </g>
    </g>

    <!-- FLECHE DE TRANSITION -->
    <g transform="translate(250, 170)">
      <path d="M0,0 L0,20 M-10,15 L0,25 L10,15" stroke="#94a3b8" stroke-width="2" fill="none"/>
    </g>

    <!-- SECTION 2 : SOFT LABELS (En bas) -->
    <g transform="translate(20, 200)">
      <rect x="0" y="0" width="460" height="130" rx="8" fill="#fffbeb" stroke="#fde68a"/>
      <text x="15" y="25" font-size="13" font-weight="600" fill="#92400e">Sortie du Teacher (Soft Target)</text>

      <!-- Barre Chien -->
      <g transform="translate(15, 45)">
        <text x="0" y="10" font-size="12" fill="#334155" font-weight="500">Chien</text>
        <!-- 90% de 350px = 315px -->
        <rect x="60" y="0" width="315" height="16" rx="3" fill="url(#softMainGrad)" />
        <text x="385" y="11" font-size="11" fill="#2563eb" font-weight="bold">90%</text>
      </g>

      <!-- Barre Chat (Dark Knowledge) -->
      <g transform="translate(15, 70)">
        <text x="0" y="10" font-size="12" fill="#334155" font-weight="500">Chat</text>
        <!-- 9% de 350px = 31.5px -->
        <rect x="60" y="0" width="32" height="16" rx="3" fill="url(#softDarkGrad)" />
        <text x="100" y="11" font-size="11" fill="#d97706" font-weight="bold">9%</text>
        
      </g>

      <!-- Barre Voiture -->
      <g transform="translate(15, 95)">
        <text x="0" y="10" font-size="12" fill="#334155" font-weight="500">Voiture</text>
        <!-- 0.1% -> ~1px de largeur minimum -->
        <rect x="60" y="0" width="2" height="16" rx="1" fill="#cbd5e1" />
        <text x="70" y="11" font-size="11" fill="#94a3b8">0.1%</text>
      </g>
    </g>

  </svg>
</div>

<p>Ces 9% pour le chat constituent la <strong>Dark Knowledge</strong>. Ils r√©v√®lent que <em>ce chien particulier ressemble un peu √† un chat</em>. Selon Hinton, cette information est cruciale car elle encode la similarit√© entre les classes. En for√ßant le petit mod√®le √† apprendre ces probabilit√©s relatives (les "Soft Targets"), on lui transf√®re bien plus d'information que le simple label "Chien". On lui apprend la structure du probl√®me, pas seulement la solution.</p>


<h2>La M√©canique : La Temp√©rature \((T)\)</h2>
<p>Pour transf√©rer efficacement cette connaissance obscure, nous devons rendre visibles les probabilit√©s faibles. Si le Teacher est trop confiant (ex: 99.99%), les nuances comme le "9% chat" sont √©cras√©es. Nous utilisons pour cela le param√®tre de <strong>Temp√©rature \((T)\)</strong> appliqu√© √† la fonction Softmax :</p>

<p>
  $$ q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)} $$ </p>

<p>La temp√©rature contr√¥le l'entropie de la distribution de sortie :</p>
<ul>
  <li><strong>\(T = 1\) :</strong> Comportement standard. La distribution est pointue, le mod√®le est tr√®s confiant.</li>
  <li><strong>\(T > 1\) :</strong> Distribution "molle". Les √©carts entre les classes se resserrent, r√©v√©lant la structure fine des donn√©es et les similarit√©s entre classes.</li>
</ul>

<p>Techniquement, Hinton sugg√®re que l'on peut voir l'apprentissage par distillation comme l'apprentissage de logits (les scores bruts) plut√¥t que de classes. Lorsque la temp√©rature est √©lev√©e, la fonction Softmax se comporte comme une r√©gression sur les logits. La temp√©rature est appliqu√©e aux logits des deux mod√®les (Teacher et Student) pour aligner leurs distributions. Les valeurs typiques oscillent g√©n√©ralement entre <strong>3 et 20</strong>.</p>

<h2>Approfondissements : Variations de la Distillation</h2>
<p>Au-del√† de l'approche standard bas√©e sur la r√©ponse finale, des techniques plus avanc√©es existent pour maximiser le transfert.</p>

<h3>La Distillation Multi-Experts</h3>
<p>Hinton souligne qu'utiliser un ensemble de mod√®les (un <em>ensemble</em>) comme Teacher est tr√®s efficace. La pr√©diction finale de l'ensemble est la moyenne (arithm√©tique ou g√©om√©trique) des pr√©dictions individuelles. L'√©l√®ve apprend alors √† reproduire cette sagesse collective. L'avantage est double : on obtient les performances de l'ensemble, mais avec la vitesse d'un seul mod√®le compact au moment de l'inf√©rence.</p>

<h3>La Distillation de Caract√©ristiques (Feature Distillation)</h3>
<p>Ici, l'√©l√®ve ne copie pas seulement la sortie finale, mais aussi les <strong>repr√©sentations interm√©diaires</strong> (les caract√©ristiques cach√©es) du Teacher. On force l'√©l√®ve √† "voir" l'image de la m√™me mani√®re que le ma√Ætre √† chaque √©tape du r√©seau.</p>
<p>Cela pose un d√©fi technique : les architectures √©tant diff√©rentes, les dimensions des couches ne correspondent pas. On utilise alors une couche lin√©aire (adaptateur) pour projeter les caract√©ristiques de l'√©l√®ve vers celles du professeur. La perte (<em>loss</em>) associ√©e repose souvent sur la <strong>similarit√© cosinus</strong> :</p>
<p>
  $$ L_{feature} = 1 - \text{CosineSimilarity}(F_{teacher}, F_{student}) $$ </p>

<h2>La Fonction de Perte : L'√âquilibre Final</h2>
<p>L'entra√Ænement de l'√©l√®ve repose sur une fonction de perte composite, √©quilibrant l'imitation du ma√Ætre et le respect de la r√©alit√© (v√©rit√© terrain) :</p>
<p>
  $$ L_{total} = \alpha \cdot L_{hard} + (1 - \alpha) \cdot L_{soft} $$ </p>
<ul>
  <li><strong>\(L_{hard}\) :</strong> Erreur classique (Cross-Entropy) par rapport aux vrais labels. Cela garde le mod√®le ancr√© dans la r√©alit√© pour ne pas qu'il d√©rive.</li>
  <li><strong>\(L_{soft}\) :</strong> Divergence (souvent KL-Divergence) entre les pr√©dictions douces du Teacher et du Student. C'est le vecteur de transfert de la "Dark Knowledge".</li>
  <li><strong>\(\alpha\) :</strong> Param√®tre de pond√©ration. Un \(\alpha\) faible privil√©gie l'imitation du ma√Ætre, ce qui est souvent pr√©f√©rable si le ma√Ætre est tr√®s performant.</li>
</ul>

<h2>R√©sultats et Perspectives</h2>
<p>La th√©orie se v√©rifie en pratique : des mod√®les √©tudiants distill√©s surpassent syst√©matiquement les mod√®les de m√™me taille entra√Æn√©s depuis z√©ro, avec un gain de performance typique de <strong>1 √† 5 %</strong>. Plus impressionnant encore, un mod√®le distill√© peut parfois surpasser le mod√®le Teacher lui-m√™me. Cela arrive lorsque le ma√Ætre, trop complexe, souffre de sur-apprentissage (<em>overfitting</em>) ; l'√©l√®ve, contraint par sa capacit√© limit√©e, est forc√© de ne retenir que l'essentiel, agissant comme un r√©gularisateur naturel.</p>
<p>Le domaine continue d'√©voluer avec des variantes prometteuses :</p>
<ul>
  <li><strong>Auto-distillation :</strong> Le mod√®le se distille lui-m√™me, agissant comme un r√©gularisateur puissant.</li>
  <li><strong>Distillation en ligne :</strong> Le Teacher et le Student apprennent simultan√©ment, s'am√©liorant mutuellement.</li>
  <li><strong>Architectures sp√©cifiques :</strong> Des mod√®les comme <strong>DistilBERT</strong> ont r√©volutionn√© le NLP en r√©duisant la taille des mod√®les de moiti√© tout en pr√©servant 97% des performances.</li>
</ul>

<h2>Lien avec le Reinforcement Learning</h2>
<p>En d√©couvrant les m√©canismes de la distillation, j'ai imm√©diatement fait le rapprochement avec le <strong>Reinforcement Learning</strong> (RL). Cette intuition m'a naturellement conduit vers le domaine de l'<strong>Apprentissage par Imitation</strong> (<em>Imitation Learning</em>).</p>
<p>Dans ce cadre, le Teacher agit exactement comme une "Politique Optimale". Ce qui m'a frapp√©, c'est la similitude entre les "Soft Labels" et le <strong>Reward Shaping</strong> en RL : au lieu d'un signal binaire et sporadique (succ√®s/√©chec), le Teacher fournit un guide dense et nuanc√©. Il n'indique pas seulement la bonne action √† l'√©l√®ve, mais aussi la valeur relative des alternatives. C'est une le√ßon de "pourquoi" et de "comment" bien plus riche qu'un simple r√©sultat.</p>
<h2>Conclusion</h2>
<p>La Distillation de Connaissances est bien plus qu'une technique de compression ; c'est un v√©ritable transfert d'intelligence. En exploitant la "Dark Knowledge" mise en lumi√®re par Hinton et en calibrant la temp√©rature du transfert, nous pouvons cr√©er des mod√®les compacts d√©tenant la sagesse des g√©ants.</p>
<h2>Liens Utiles</h2>
<p>Pour approfondir vos connaissances et explorer des outils avanc√©s, voici quelques ressources :</p>
<ul>
  <li><a href="https://www.ttic.edu/dl/dark14.pdf">Dark knowledge (Geoffrey Hinton, Oriol Vinyals & Jeff Dean)</a></li>
  <li><a href="https://github.com/MENA-ML/lab_sessions_2026/tree/main/distillation">Knowledge Distillation: Teacher-Student Networks by  Adam Kosiorek & Ruba Haroun (Google DeepMind), Andres Villa (KAUST) (MENA-ML)</a></li>
  <li><a href="https://www.ibm.com/fr-fr/think/topics/knowledge-distillation">Qu‚Äôest-ce que la distillation de connaissances ? (IBM)</a></li>
  <li><a href="https://www.llama.com/docs/how-to-guides/distillation/">Distillation (Llama by Meta)</a></li>
</ul>
<div class="music-suggestion">
  <p>Ma recommandation musicale du jour : √† √©couter sans mod√©ration !</p>
  <a href="https://www.youtube.com/watch?v=Cmh_uRzDVfw" target="_blank">√âcouter sur YouTube</a>
</div>
<footer>
  <p><span class="copyleft">&copy;</span> 2026 Jonathan Suru. This work is free.</p>
</footer>
<!-- 100% privacy-first analytics -->
<script data-collect-dnt="true" async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=true" alt="" referrerpolicy="no-referrer-when-downgrade"/></noscript>

</body>
</html>